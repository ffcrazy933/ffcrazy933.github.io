---
abstract: In everyday life, humans often plan their actions by following
  step-by-step instructions in the form of goal-oriented scripts. Previous work
  has exploited language models (LMs) to plan for abstract goals of
  stereotypical activities (e.g., "make a cake"), but leaves more specific goals
  with multi-facet constraints understudied (e.g., "make a cake for diabetics").
  In this paper, we define the task of constrained language planning for the
  first time. We propose an overgenerate-then-filter approach to improve large
  language models (LLMs) on this task, and use it to distill a novel constrained
  language planning dataset, CoScript, which consists of 55,000 scripts.
  Empirical results demonstrate that our method significantly improves the
  constrained language planning ability of LLMs, especially on constraint
  faithfulness. Furthermore, CoScript is demonstrated to be quite effective in
  endowing smaller LMs with constrained language planning ability.
slides: ""
url_pdf: https://arxiv.org/abs/2305.05252
publication_types:
  - "1"
authors:
  - admin
  - Jiangjie Chen
  - Ziquan Fu
  - Xuyang Ge
  - Soham Shah
  - Charles Robert Jankowski
  - Deqing Yang
  - Yanghua Xiao
author_notes: []
publication: In *The 61th Annual Meeting of the Association for Computational
  Linguistics (**ACL 2023**)*
summary: ""
url_dataset: ""
url_project: ""
publication_short: ""
url_source: ""
url_video: ""
title: Distilling Script Knowledge from Large Language Models for Constrained
  Language Planning
doi: ""
featured: false
tags: []
projects: []
image:
  caption: ""
  focal_point: ""
  preview_only: false
  filename: 2.png
date: 2023-05-10T02:12:23.523Z
url_slides: ""
publishDate: 2017-01-01T00:00:00.000Z
url_poster: ""
url_code: https://github.com/siyuyuan/coscript
---
