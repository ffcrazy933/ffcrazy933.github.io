[{"authors":null,"categories":null,"content":"Siyu Yuan (员司雨) is a second-year Ph.D. studentat the School of Data Science in Fudan University and a member of KW FUDAN Lab, supervised by Associate Prof. Deqing Yang and Prof. Yanghua Xiao.\nShe is devoted to acquiring knowledge, especially meta-knowledge based on pre-trained language models and making machines have the human-like cognitive abilities. Main interested research topics include (but not limited to)\nSymbolic knowledge distillation, especially distilling symbolic knowledge from large language models (LLMs) and transferring it to smaller but specialized models to enhance their performance. LLM prompt engineering, especially on endow appropriate instructions with COT to LLMs (e.g., GPT-3, BLOOM, BLOOMZ, LLaMA) for various kinds of task. Knowledge acquisition, especially on excavating knowledge based on pre-trained language models, including concept acquisition, script generation and analogy making. (Download my resumé.)\n","date":1683684743,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1683684743,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"Siyu Yuan (员司雨) is a second-year Ph.D. studentat the School of Data Science in Fudan University and a member of KW FUDAN Lab, supervised by Associate Prof. Deqing Yang and Prof.","tags":null,"title":"Siyu Yuan","type":"authors"},{"authors":["Siyu Yuan","Deqing Yang","Jinxi Liu","Shuyu Tian","Jiaqing Liang","Yanghua Xiao","Rui Xie"],"categories":null,"content":"","date":1683684743,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1683684743,"objectID":"2a376be39e1a54cb1a64d43712348e29","permalink":"https://example.com/publication/causality-aware-concept-extraction-based-on-knowledge-guided-prompting/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/causality-aware-concept-extraction-based-on-knowledge-guided-prompting/","section":"publication","summary":"we propose equipping the PLM-based extractor with a knowledge-guided prompt as an intervention to alleviate concept bias through the lens of a Structural Causal Model (SCM). The prompt adopts the topic of the given entity from the existing knowledge in KGs to mitigate the spurious co-occurrence correlations between entities and biased concepts.","tags":[],"title":"Causality-aware Concept Extraction based on Knowledge-guided Prompting","type":"publication"},{"authors":["Siyu Yuan","Jiangjie Chen","Ziquan Fu","Xuyang Ge","Soham Shah","Charles Robert Jankowski","Deqing Yang","Yanghua Xiao"],"categories":null,"content":"","date":1683684743,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1683684743,"objectID":"36e2d6c3773599d271e3029ef28e6030","permalink":"https://example.com/publication/distilling-script-knowledge-from-large-language-models-for-constrained-language-planning/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/distilling-script-knowledge-from-large-language-models-for-constrained-language-planning/","section":"publication","summary":"We propose an overgenerate-then-filter approach to improve large language models (LLMs) on this task, and use it to distill a novel constrained language planning dataset, CoScript, which consists of 55,000 scripts.","tags":[],"title":"Distilling Script Knowledge from Large Language Models for Constrained Language Planning","type":"publication"},{"authors":["Siyu Yuan","Deqing Yang","Jiaqing Liang","Zhixu Li","Jinxi Liu","Jingyue Huang","Yanghua Xiao Long paper"],"categories":null,"content":"","date":1670379143,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1670379143,"objectID":"d38f47e1f636416781fdb11b1434189d","permalink":"https://example.com/publication/generative-entity-typing-with-curriculum-learning/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/generative-entity-typing-with-curriculum-learning/","section":"publication","summary":"We employ curriculum learning (CL) to train a generative entity typing model to overcome issues with coarse-grained types and heterogeneous data, where the curriculum could be self-adjusted with the self-paced learning according to its comprehension of the type granularity and data heterogeneity.","tags":[],"title":"Generative Entity Typing with Curriculum Learning","type":"publication"},{"authors":["Jingjie Yi","Deqing Yang","Siyu Yuan","Kaiyan Cao","Zhiyao Zhang and Yanghua Xiao"],"categories":null,"content":"","date":1645236743,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1645236743,"objectID":"5fbe347274a01bdbf1fcd2005ea98797","permalink":"https://example.com/publication/contextual-information-and-commonsense-based-prompt-for-emotion-recognition-in-conversation/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/contextual-information-and-commonsense-based-prompt-for-emotion-recognition-in-conversation/","section":"publication","summary":"Emotion recognition in conversation (ERC) aims to detect the emotion for each utterance in a given conversation. The newly proposed ERC models have leveraged pre-trained language models (PLMs) with the paradigm of pre-training and fine-tuning to obtain good performance. However, these models seldom exploit PLMs' advantages thoroughly, and perform poorly for the conversations lacking explicit emotional expressions. In order to fully leverage the latent knowledge related to the emotional expressions in utterances, we propose a novel ERC model CISPER with the new paradigm of prompt and language model (LM) tuning. Specifically, CISPER is equipped with the prompt blending the contextual information and commonsense related to the interlocutor's utterances, to achieve ERC more effectively. Our extensive experiments demonstrate CISPER's superior performance over the state-of-the-art ERC models, and the effectiveness of leveraging these two kinds of significant prompt information for performance gains. To reproduce our experimental results conveniently, CISPER's sourcecode and the datasets have been shared at https://github.com/DeqingYang/CISPER.","tags":[],"title":"Contextual Information and Commonsense Based Prompt for Emotion Recognition in Conversation","type":"publication"},{"authors":["Siyu Yuan","Deqing Yang","Jiaqing Liang","Jilun Sun","Jingyue Huang","Kaiyan Cao","Yanghua Xiao","Rui Xie"],"categories":null,"content":"","date":1635042840,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635042840,"objectID":"80d7b171c636e2d481fef3a7529380d7","permalink":"https://example.com/publication/large-scale-multi-granular-concept-extraction-based-on-machine-reading-comprehension/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/large-scale-multi-granular-concept-extraction-based-on-machine-reading-comprehension/","section":"publication","summary":"The concepts in knowledge graphs (KGs) enable machines to understand natural language, and thus play an indispensable role in many applications. However, existing KGs have the poor coverage of concepts, especially fine-grained concepts. In order to supply existing KGs with more fine-grained and new concepts, we propose a novel concept extraction framework, namely MRC-CE, to extract large-scale multi-granular concepts from the descriptive texts of entities. Specifically, MRC-CE is built with a machine reading comprehension model based on BERT, which can extract more fine-grained concepts with a pointer network. Furthermore, a random forest and rule-based pruning are also adopted to enhance MRC-CE's precision and recall simultaneously. Our experiments evaluated upon multilingual KGs, i.e., English Probase and Chinese CN-DBpedia, justify MRC-CE's superiority over the state-of-the-art extraction models in KG completion. Particularly, after running MRC-CE for each entity in CN-DBpedia, more than 7,053,900 new concepts (instanceOf relations) are supplied into the KG. The code and datasets have been released at https://github.com/fcihraeipnusnacwh/MRC-CE.","tags":[],"title":"Large-Scale Multi-granular Concept Extraction Based on Machine Reading Comprehension","type":"publication"},{"authors":[],"categories":[],"content":"Create slides in Markdown with Wowchemy Wowchemy | Documentation\nFeatures Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides Controls Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026#34;blueberry\u0026#34; if porridge == \u0026#34;blueberry\u0026#34;: print(\u0026#34;Eating...\u0026#34;) Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\nFragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}} Press Space to play!\nOne Two Three A fragment can accept two optional parameters:\nclass: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}} Press the S key to view the speaker notes!\nOnly the speaker can read these notes Press S key to view Themes black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026#34;/media/boards.jpg\u0026#34; \u0026gt;}} {{\u0026lt; slide background-color=\u0026#34;#0000FF\u0026#34; \u0026gt;}} {{\u0026lt; slide class=\u0026#34;my-style\u0026#34; \u0026gt;}} Custom CSS Example Let’s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; } Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"https://example.com/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Wowchemy's Slides feature.","tags":[],"title":"Slides","type":"slides"}]